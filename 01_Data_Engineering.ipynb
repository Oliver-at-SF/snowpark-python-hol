{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "We begin where all ML use cases do: data engineering. In this section of the demo, we will utilize Snowpark's Python client-side Dataframe API to build an **ELT pipeline**.  We will extract the data from the source system (s3), load it into snowflake and add transformations to clean the data before analysis. \n",
    "\n",
    "The data engineer has been told that there is historical data going back to 2013 and new data will be made available at the end of each month. \n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The data engineer can develop in their tool of choice (ie. VS Code, IntelliJ, Pycharm, Eclipse, etc.).  Snowpark Python makes it possible to use any environment where they have a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the data engineer can use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models.  \n",
    "\n",
    "Input: Historical bulk data at `https://s3.amazonaws.com/tripdata/`. Incremental data to be loaded one month at a time.  \n",
    "Output: `trips` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(level=logging.WARN)\n",
    "#logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will utilize a simple json file to store our credentials. This should **never** be done in production and is for demo purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, compute_parameters, state_dict = snowpark_connect('./include/creds.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='LOAD_STAGE already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fq_load_stage_name=session.get_fully_qualified_current_schema()+'.\\\"'+state_dict['load_stage_name']+'\\\"'\n",
    "#session.sql('CREATE STAGE IF NOT EXISTS '+fq_load_stage_name).collect()\n",
    "load_stage_name=state_dict['load_stage_name']\n",
    "session.sql('CREATE STAGE IF NOT EXISTS '+load_stage_name).collect()\n",
    "#session.sql('CREATE OR REPLACE STAGE '+state_dict['load_stage_name']).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of files to download and upload to stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#For files like 201306-citibike-tripdata.zip\n",
    "date_range1 = pd.period_range(start=datetime.strptime(\"201306\", \"%Y%m\"), \n",
    "                             end=datetime.strptime(\"201612\", \"%Y%m\"), \n",
    "                             freq='M').strftime(\"%Y%m\")\n",
    "file_name_end1 = '-citibike-tripdata.zip'\n",
    "files_to_download = [date+file_name_end1 for date in date_range1.to_list()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting in January 2017 Citibike changed the format of the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For files like 201701-citibike-tripdata.csv.zip\n",
    "date_range2 = pd.period_range(start=datetime.strptime(\"201701\", \"%Y%m\"), \n",
    "                             end=datetime.strptime(\"202112\", \"%Y%m\"), \n",
    "                             freq='M').strftime(\"%Y%m\")\n",
    "file_name_end2 = '-citibike-tripdata.csv.zip'\n",
    "files_to_download = files_to_download + [date+file_name_end2 for date in date_range2.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For development purposes we will start with loading just a couple of files.  We will create a bulk load process afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['201306-citibike-tripdata.zip', '202112-citibike-tripdata.csv.zip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_download = [files_to_download[i] for i in [0,102]] #19,50,100,102]]\n",
    "files_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse(compute_parameters['fe_warehouse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1_download_files = list()\n",
    "schema2_download_files = list()\n",
    "schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "\n",
    "for file_name in files_to_download:\n",
    "    file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "    if file_start_date < schema2_start_date:\n",
    "        schema1_download_files.append(file_name)\n",
    "    else:\n",
    "        schema2_download_files.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['201306-citibike-tripdata.zip'], ['202112-citibike-tripdata.csv.zip'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema1_download_files, schema2_download_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/201306-citibike-tripdata.zip\n",
      "Putting 201306-citibike-tripdata.csv to stage: LOAD_STAGE/schema1/\n",
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/202112-citibike-tripdata.csv.zip\n",
      "Putting 202112-citibike-tripdata.csv to stage: LOAD_STAGE/schema2/\n"
     ]
    }
   ],
   "source": [
    "schema1_load_stage = state_dict['load_stage_name']+'/schema1/'\n",
    "schema2_load_stage = state_dict['load_stage_name']+'/schema2/'\n",
    "\n",
    "schema1_files_to_load = list()\n",
    "for zip_file_name in schema1_download_files:\n",
    "    \n",
    "    url = state_dict['download_base_url']+zip_file_name\n",
    "    \n",
    "    print('Downloading and unzipping: '+url)\n",
    "    r = requests.get(url)\n",
    "    file = ZipFile(BytesIO(r.content))\n",
    "    csv_file_name=file.namelist()[0]\n",
    "    file.extract(csv_file_name)\n",
    "    file.close()\n",
    "    \n",
    "    print('Putting '+csv_file_name+' to stage: '+schema1_load_stage)\n",
    "    session.file.put(local_file_name=csv_file_name, \n",
    "                     stage_location=schema1_load_stage, \n",
    "                     source_compression='NONE', \n",
    "                     overwrite=True)\n",
    "    schema1_files_to_load.append(csv_file_name)\n",
    "    os.remove(csv_file_name)\n",
    "    \n",
    "schema2_files_to_load = list()\n",
    "for zip_file_name in schema2_download_files:\n",
    "    \n",
    "    url = state_dict['download_base_url']+zip_file_name\n",
    "    \n",
    "    print('Downloading and unzipping: '+url)\n",
    "    r = requests.get(url)\n",
    "    file = ZipFile(BytesIO(r.content))\n",
    "    csv_file_name=file.namelist()[0]\n",
    "    file.extract(csv_file_name)\n",
    "    file.close()\n",
    "    \n",
    "    print('Putting '+csv_file_name+' to stage: '+schema2_load_stage)\n",
    "    session.file.put(local_file_name=csv_file_name, \n",
    "                     stage_location=schema2_load_stage, \n",
    "                     source_compression='NONE', \n",
    "                     overwrite=True)\n",
    "    schema2_files_to_load.append(csv_file_name)\n",
    "    os.remove(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='load_stage/schema1/201306-citibike-tripdata.csv.gz', size=16218896, md5='bd979640f17f10a3bf42f449aff29ad6', last_modified='Mon, 21 Feb 2022 23:51:40 GMT'),\n",
       " Row(name='load_stage/schema2/202112-citibike-tripdata.csv.gz', size=60670624, md5='f32f0bd73c8304fda217fa4b9aa554fa', last_modified='Mon, 21 Feb 2022 23:52:09 GMT')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"list @\"+load_stage_name+\" pattern='.*20.*[.]gz'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load: \n",
    "Load raw as all string type.  We will fix data types in the transform stage.\n",
    "\n",
    "There are two schema types so we will create two ingest tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upper case fields are common to both schemas.\n",
    "#Schema from 2013 to 2021\n",
    "load_schema1 = T.StructType([T.StructField(\"tripduration\", T.StringType()),\n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"bike_id\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType()), \n",
    "                             T.StructField(\"birth_year\", T.StringType()),\n",
    "                             T.StructField(\"gender\", T.StringType())])\n",
    "\n",
    "#starting in February 2021 the schema changed\n",
    "load_schema2 = T.StructType([T.StructField(\"ride_id\", T.StringType()), \n",
    "                             T.StructField(\"rideable_type\", T.StringType()), \n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])\n",
    "\n",
    "trips_table_schema = T.StructType([T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty tables in order to setup CDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.createDataFrame([[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]], \n",
    "                        schema=load_schema1)\\\n",
    "       .na.drop()\\\n",
    "       .write.mode('overwrite')\\\n",
    "       .saveAsTable(state_dict['load_table_name']+'schema1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.createDataFrame([[None, None, None, None, None, None, None, None, None, None, None, None, None]], \n",
    "                        schema=load_schema2)\\\n",
    "       .na.drop()\\\n",
    "       .write.mode('overwrite')\\\n",
    "       .saveAsTable(state_dict['load_table_name']+'schema2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "#                      .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "#                      .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "#                      .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "#                      .option(\"NULL_IF\", \"NULL\")\\\n",
    "#                      .option(\"pattern\", \"'.*20.*[.]gz'\")\\\n",
    "#                      .schema(load_schema1)\\\n",
    "#                      .csv(load_stage_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Pipe SCHEMA1_PIPE successfully created.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema1_pipe_name = 'SCHEMA1_PIPE'\n",
    "schema1_load_table = state_dict['load_table_name']+'schema1'\n",
    "session.sql('CREATE OR REPLACE PIPE '+schema1_pipe_name+\\\n",
    "            ' AS COPY INTO '+schema1_load_table+\\\n",
    "            ' FROM @'+schema1_load_stage).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Pipe SCHEMA2_PIPE successfully created.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema2_pipe_name = 'SCHEMA2_PIPE'\n",
    "schema2_load_table = state_dict['load_table_name']+'schema2'\n",
    "session.sql('CREATE OR REPLACE PIPE '+schema2_pipe_name+\\\n",
    "              ' AS COPY INTO '+schema2_load_table+' FROM @'+schema2_load_stage).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q snowflake-ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cryptography.hazmat.primitives import serialization as crypto_serialization\n",
    "# from cryptography.hazmat.primitives.asymmetric import rsa\n",
    " \n",
    "# keySize = 2048\n",
    " \n",
    "# key = rsa.generate_private_key(public_exponent=65537, key_size=keySize)\n",
    " \n",
    "# private_key = key.private_bytes(\n",
    "#     crypto_serialization.Encoding.PEM,\n",
    "#     crypto_serialization.PrivateFormat.PKCS8,\n",
    "#     crypto_serialization.NoEncryption()\n",
    "# )\n",
    "# private_key = private_key.decode('utf-8')\n",
    " \n",
    "# public_key = key.public_key().public_bytes(\n",
    "#     crypto_serialization.Encoding.PEM,\n",
    "#     crypto_serialization.PublicFormat.SubjectPublicKeyInfo\n",
    "# )\n",
    "# public_key = public_key.decode('utf-8')\n",
    "\n",
    "# public_key_str=''.join(public_key.split('\\n')[1:-2])\n",
    "# private_key_str=''.join(private_key.split('\\n')[1:-2])\n",
    "\n",
    "# with open('citibike-ml-john-private-key.pem', 'w') as fh:\n",
    "#     fh.write(private_key)\n",
    "# with open('citibike-ml-john-public-key.pem', 'w') as fh:\n",
    "#     fh.write(public_key)\n",
    "\n",
    "# session.use_role('ACCOUNTADMIN')\n",
    "# session.sql('ALTER USER '+connection_parameters['user']+' SET RSA_PUBLIC_KEY=\\\"'+public_key_str+'\\\"').collect()\n",
    "# session.use_role('DBA_CITIBIKE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ingest import SimpleIngestManager\n",
    "from snowflake.ingest import StagedFile\n",
    "from snowflake.ingest.utils.uris import DEFAULT_SCHEME\n",
    "import json\n",
    "with open('include/creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    connection_parameters = {\n",
    "      'account': data['account'],\n",
    "      'user': data['username'],\n",
    "      'password': data['password'],\n",
    "      'role': data['role'],\n",
    "      'schema': data['schema'],\n",
    "      'database': data['database'],\n",
    "      'warehouse': data['warehouse']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_pem_private_key() missing 1 required positional argument: 'password'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./citibike-ml-john-private-key.pem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pem_in:\n\u001b[1;32m      9\u001b[0m   pemlines \u001b[38;5;241m=\u001b[39m pem_in\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 10\u001b[0m   private_key_obj \u001b[38;5;241m=\u001b[39m \u001b[43mload_pem_private_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpemlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m private_key_text \u001b[38;5;241m=\u001b[39m private_key_obj\u001b[38;5;241m.\u001b[39mprivate_bytes(\n\u001b[1;32m     13\u001b[0m   Encoding\u001b[38;5;241m.\u001b[39mPEM, PrivateFormat\u001b[38;5;241m.\u001b[39mPKCS8, NoEncryption())\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m private_key_text\n",
      "\u001b[0;31mTypeError\u001b[0m: load_pem_private_key() missing 1 required positional argument: 'password'"
     ]
    }
   ],
   "source": [
    "# with open('citibike-ml-john-private-key.pem', 'r') as fh:\n",
    "#     private_key=fh.read()\n",
    "# private_key_str=''.join(private_key.split('\\n')[1:-2])\n",
    "from cryptography.hazmat.primitives.serialization import load_pem_private_key\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "import os\n",
    "with open(\"./citibike-ml-john-private-key.pem\", 'rb') as pem_in:\n",
    "  pemlines = pem_in.read()\n",
    "  private_key_obj = load_pem_private_key(pemlines)\n",
    "\n",
    "private_key_text = private_key_obj.private_bytes(\n",
    "  Encoding.PEM, PrivateFormat.PKCS8, NoEncryption()).decode('utf-8')\n",
    "\n",
    "private_key_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1_ingest_manager = SimpleIngestManager(account=connection_parameters['account'],\n",
    "                                             host=connection_parameters['account']+'.snowflakecomputing.com',\n",
    "                                             user=connection_parameters['user'],\n",
    "                                             pipe=schema1_pipe_name,\n",
    "                                             private_key=private_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_file_list = []\n",
    "for file_name in schema1_files_to_load:\n",
    "    staged_file_list.append(StagedFile(file_name, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_on=datetime.datetime(2022, 2, 21, 15, 54, 6, 165000, tzinfo=<DstTzInfo 'America/Los_Angeles' PST-1 day, 16:00:00 STD>), name='SCHEMA1_PIPE', database_name='CITIBIKEML', schema_name='DEMO', definition='COPY INTO RAW_schema1 FROM @LOAD_STAGE/schema1/', owner='DBA_CITIBIKE', notification_channel=None, comment='', integration=None, pattern=None, error_integration=None)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('describe pipe '+schema1_pipe_name).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IngestResponseError",
     "evalue": "Http Error: 404, Vender Code: 390404, Message: Specified object does not exist or not authorized. Pipe not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIngestResponseError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mschema1_ingest_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaged_file_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/snowpark_040/lib/python3.8/site-packages/snowflake/ingest/simple_ingest_manager.py:125\u001b[0m, in \u001b[0;36mSimpleIngestManager.ingest_files\u001b[0;34m(self, staged_files, request_id)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Send our request!\u001b[39;00m\n\u001b[1;32m    124\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_headers()\n\u001b[0;32m--> 125\u001b[0m response_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestful\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIngest response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(response_body))\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_body\n",
      "File \u001b[0;32m/opt/anaconda3/envs/snowpark_040/lib/python3.8/site-packages/snowflake/ingest/utils/network.py:33\u001b[0m, in \u001b[0;36mSnowflakeRestful.post\u001b[0;34m(self, url, json, headers)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: Text, json: Dict, headers: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[Text, Any]:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Http POST request\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    :param url: request url,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    :return: response payload\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_request_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/snowpark_040/lib/python3.8/site-packages/snowflake/ingest/utils/network.py:83\u001b[0m, in \u001b[0;36mSnowflakeRestful._exec_request_with_retry\u001b[0;34m(self, url, method, headers, json)\u001b[0m\n\u001b[1;32m     80\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(next_sleep_time)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IngestResponseError(response)\n",
      "\u001b[0;31mIngestResponseError\u001b[0m: Http Error: 404, Vender Code: 390404, Message: Specified object does not exist or not authorized. Pipe not found"
     ]
    }
   ],
   "source": [
    "schema1_ingest_manager.ingest_files(staged_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from datetime import timedelta\n",
    "# import datetime\n",
    "\n",
    "# try:\n",
    "#     resp = schema1_ingest_manager.ingest_files(staged_file_list)\n",
    "# except HTTPError as e:\n",
    "#     # HTTP error, may need to retry\n",
    "#     logger.error(e)\n",
    "#     exit(1)\n",
    "\n",
    "# # This means Snowflake has received file and will start loading\n",
    "# assert(resp['responseCode'] == 'SUCCESS')\n",
    "\n",
    "# # Needs to wait for a while to get result in history\n",
    "# while True:\n",
    "#     history_resp = schema1_ingest_manager.get_history()\n",
    "\n",
    "#     if len(history_resp['files']) > 0:\n",
    "#         print('Ingest Report:\\n')\n",
    "#         print(history_resp)\n",
    "#         break\n",
    "#     else:\n",
    "#         # wait for 20 seconds\n",
    "#         time.sleep(20)\n",
    "\n",
    "#     hour = timedelta(hours=1)\n",
    "#     date = datetime.datetime.utcnow() - hour\n",
    "#     history_range_resp = schema1_ingest_manager.get_history_range(date.isoformat() + 'Z')\n",
    "\n",
    "#     print('\\nHistory scan report: \\n')\n",
    "#     print(history_range_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='load_stage/schema1/201306-citibike-tripdata.csv.gz', size=16218896, md5='bd979640f17f10a3bf42f449aff29ad6', last_modified='Mon, 21 Feb 2022 23:51:40 GMT')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('list @LOAD_STAGE/schema1/').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"DBA_CITIBIKE\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.get_current_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trips_to_raw(session, files_to_load:list, load_stage_name:str, load_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from snowflake.snowpark import types as T\n",
    "    from datetime import datetime\n",
    "\n",
    "    stage_table_names = list()\n",
    "    schema1_files = list()\n",
    "    schema2_files = list()\n",
    "    schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "    \n",
    "    for file_name in files_to_load:\n",
    "        file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "        if file_start_date < schema2_start_date:\n",
    "            schema1_files.append(file_name)\n",
    "        else:\n",
    "            schema2_files.append(file_name)\n",
    "\n",
    "    if len(schema1_files) > 0:\n",
    "        load_schema1 = asdfasdf\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "        \n",
    "        stage_table_name = load_table_name + str('schema1')\n",
    "        \n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                              .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                              .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                              .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                              .option(\"NULL_IF\", \"NULL\")\\\n",
    "                              .schema(load_schema1)\\\n",
    "                              .csv(\"@\"+load_stage_name)\\\n",
    "                              .copy_into_table(stage_table_name, \n",
    "                                               files=schema1_files, \n",
    "                                               format_type_options=csv_file_format_options)\n",
    "        stage_table_names.append(stage_table_name)\n",
    "\n",
    "\n",
    "    if len(schema2_files) > 0:\n",
    "        load_schema2 = asdfasdf\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "\n",
    "        stage_table_name = load_table_name + str('schema2')\n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                              .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                              .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                              .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                              .option(\"NULL_IF\", \"NULL\")\\\n",
    "                              .schema(load_schema2)\\\n",
    "                              .csv(\"@\"+load_stage_name)\\\n",
    "                              .copy_into_table(stage_table_name, \n",
    "                                               files=schema2_files, \n",
    "                                               format_type_options=csv_file_format_options)\n",
    "        stage_table_names.append(stage_table_name)\n",
    "        \n",
    "    return list(set(stage_table_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "interim_target_table_names = list()\n",
    "for stage_table_name in stage_table_names:\n",
    "    schema = stage_table_name.split(\"_\")[1]\n",
    "    if schema == 'schema1':\n",
    "        interim_target_table_name = 'INTERIM_schema1'\n",
    "        stream_name = 'STREAM_schema1'\n",
    "        task_name = 'TRIPSCDCTASK_schema1'\n",
    "        procedure_name = 'TRIPSCDCPROC_schema1'\n",
    "        create_processcdc_procedure_statement = schema1_spoc_str(procedure_name, \n",
    "                                                                 interim_target_table_name, \n",
    "                                                                 stream_name)\n",
    "\n",
    "    elif schema == 'schema2':\n",
    "        interim_target_table_name = 'INTERIM_schema2'\n",
    "        stream_name = 'STREAM_schema2'\n",
    "        task_name = 'TRIPSCDCTASK_schema2'\n",
    "        procedure_name = 'TRIPSCDCPROC_schema2'\n",
    "        create_processcdc_procedure_statement = schema2_spoc_str(procedure_name, \n",
    "                                                                 interim_target_table_name, \n",
    "                                                                 stream_name)\n",
    "\n",
    "    #outside the if else condition but still inside the for loop\n",
    "    interim_target_table_names.append(interim_target_table_name)\n",
    "    create_stream_sql ='CREATE OR REPLACE STREAM ' + stream_name + \\\n",
    "                   ' ON TABLE ' + stage_table_name + \\\n",
    "                   ' APPEND_ONLY = FALSE SHOW_INITIAL_ROWS = TRUE'\n",
    "\n",
    "    create_interim_target_table_sql = 'CREATE OR REPLACE TABLE ' + interim_target_table_name +\\\n",
    "                                ' LIKE ' + stage_table_name\n",
    "    create_task_statement = \"CREATE OR REPLACE TASK \" + task_name + \\\n",
    "                        \" WAREHOUSE='\" + cdc_task_warehouse_name +\"'\"+ \\\n",
    "                        \" SCHEDULE = '1 minute'\"+ \\\n",
    "                        \" WHEN SYSTEM$STREAM_HAS_DATA('\" + stream_name + \"')\"+\\\n",
    "                        \" AS CALL \" + procedure_name + \"()\"\n",
    "    resume_task_statement = \"ALTER TASK \" + task_name + \" RESUME\"\n",
    "\n",
    "    _ = session.sql(create_stream_sql).collect()\n",
    "    _ = session.sql(create_interim_target_table_sql).collect() \n",
    "    _ = session.sql(create_processcdc_procedure_statement).collect()\n",
    "    _ = session.sql(create_task_statement).collect()\n",
    "    _ = session.sql(resume_task_statement).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "\n",
    "print('Loading '+str(loaddf.count())+' records to table '+state_dict['load_table_name']+str('schema1'))\n",
    "loaddf.copy_into_table(state_dict['load_table_name']+str('schema1'), \n",
    "                       files=files_to_load, \n",
    "                       format_type_options=csv_file_format_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transform:\n",
    "We have the raw data loaded. Now let's transform this data and clean it up. This will push the data to a final \\\"transformed\\\" table to be consumed by our Data Science team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transdf = session.table(state_dict['load_table_name']+'schema1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different date formats \"2014-08-10 15:21:22\", \"1/1/2015 1:30\" and \"12/1/2014 02:04:53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format_2 = \"1/1/2015 [0-9]:.*$\"      #1/1/2015 1:30 -> #M*M/D*D/YYYY H*H:M*M(:SS)*\n",
    "date_format_3 = \"1/1/2015 [0-9][0-9]:.*$\" #1/1/2015 10:30 -> #M*M/D*D/YYYY H*H:M*M(:SS)*\n",
    "date_format_4 = \"12/1/2014.*\"             #12/1/2014 02:04:53 -> M*M/D*D/YYYY \n",
    "\n",
    "#Change all dates to YYYY-MM-DD HH:MI:SS format\n",
    "date_format_match = \"^([0-9]?[0-9])/([0-9]?[0-9])/([0-9][0-9][0-9][0-9]) ([0-9]?[0-9]):([0-9][0-9])(:[0-9][0-9])?.*$\"\n",
    "date_format_repl = \"\\\\3-\\\\1-\\\\2 \\\\4:\\\\5\\\\6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transdf.withColumn('STARTTIME', F.regexp_replace(F.col('STARTTIME'),\n",
    "                                            F.lit(date_format_match), \n",
    "                                            F.lit(date_format_repl)))\\\n",
    "      .withColumn('STARTTIME', F.to_timestamp('STARTTIME'))\\\n",
    "      .withColumn('STOPTIME', F.regexp_replace(F.col('STOPTIME'),\n",
    "                                            F.lit(date_format_match), \n",
    "                                            F.lit(date_format_repl)))\\\n",
    "      .withColumn('STOPTIME', F.to_timestamp('STOPTIME'))\\\n",
    "      .select(F.col('STARTTIME'), \n",
    "              F.col('STOPTIME'), \n",
    "              F.col('START_STATION_ID'), \n",
    "              F.col('START_STATION_NAME'), \n",
    "              F.col('START_STATION_LATITUDE'), \n",
    "              F.col('START_STATION_LONGITUDE'), \n",
    "              F.col('END_STATION_ID'), \n",
    "              F.col('END_STATION_NAME'), F.col('END_STATION_LATITUDE'), \n",
    "              F.col('END_STATION_LONGITUDE'), \n",
    "              F.col('USERTYPE'))\\\n",
    "      .write.mode('overwrite').saveAsTable(state_dict['trips_table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = session.table(state_dict['trips_table_name'])\n",
    "testdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export code in functional modules for MLOps and orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/elt.py\n",
    "def schema1_definition():\n",
    "    from snowflake.snowpark import types as T\n",
    "    load_schema1 = T.StructType([T.StructField(\"TRIPDURATION\", T.StringType()),\n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"BIKEID\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType()), \n",
    "                             T.StructField(\"BIRTH_YEAR\", T.StringType()),\n",
    "                             T.StructField(\"GENDER\", T.StringType())])\n",
    "    return load_schema1\n",
    "\n",
    "def schema2_definition():\n",
    "    from snowflake.snowpark import types as T\n",
    "    load_schema2 = T.StructType([T.StructField(\"ride_id\", T.StringType()), \n",
    "                             T.StructField(\"rideable_type\", T.StringType()), \n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])\n",
    "    return load_schema2\n",
    "\n",
    "def conformed_schema():\n",
    "    from snowflake.snowpark import types as T\n",
    "    trips_table_schema = T.StructType([T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])\n",
    "    return trips_table_schema\n",
    "\n",
    "\n",
    "def setup_cdc():\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def extract_trips_to_stage(session, files_to_download: list, download_base_url: str, load_stage_name:str):\n",
    "    import os \n",
    "    import requests\n",
    "    from zipfile import ZipFile\n",
    "    import gzip\n",
    "    \n",
    "    files_to_load = list()\n",
    "    \n",
    "    for zip_file_name in files_to_download:\n",
    "        gz_file_name = os.path.splitext(zip_file_name)[0]+'.gz'\n",
    "        url = download_base_url+zip_file_name\n",
    "\n",
    "        print('Downloading file '+url)\n",
    "        r = requests.get(url)\n",
    "        with open(zip_file_name, 'wb') as fh:\n",
    "            fh.write(r.content)\n",
    "\n",
    "        with ZipFile(zip_file_name, 'r') as zipObj:\n",
    "            csv_file_names = zipObj.namelist()\n",
    "            with zipObj.open(name=csv_file_names[0], mode='r') as zf:\n",
    "                print('Gzipping file '+csv_file_names[0])\n",
    "                with gzip.open(gz_file_name, 'wb') as gzf:\n",
    "                    gzf.write(zf.read())\n",
    "\n",
    "        print('Putting file '+gz_file_name+' to stage '+load_stage_name)\n",
    "        session.file.put(gz_file_name, '@'+load_stage_name)\n",
    "        \n",
    "        files_to_load.append(gz_file_name)\n",
    "        os.remove(zip_file_name)\n",
    "        os.remove(gz_file_name)\n",
    "    \n",
    "    return load_stage_name, files_to_load\n",
    "        \n",
    "def load_trips_to_raw(session, files_to_load:list, load_stage_name:str, load_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from snowflake.snowpark import types as T\n",
    "    from datetime import datetime\n",
    "\n",
    "    stage_table_names = list()\n",
    "    schema1_files = list()\n",
    "    schema2_files = list()\n",
    "    schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "    \n",
    "    for file_name in files_to_load:\n",
    "        file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "        if file_start_date < schema2_start_date:\n",
    "            schema1_files.append(file_name)\n",
    "        else:\n",
    "            schema2_files.append(file_name)\n",
    "\n",
    "    if len(schema1_files) > 0:\n",
    "        load_schema1 = asdfasdf\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "        \n",
    "        stage_table_name = load_table_name + str('schema1')\n",
    "        \n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                              .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                              .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                              .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                              .option(\"NULL_IF\", \"NULL\")\\\n",
    "                              .schema(load_schema1)\\\n",
    "                              .csv(\"@\"+load_stage_name)\\\n",
    "                              .copy_into_table(stage_table_name, \n",
    "                                               files=schema1_files, \n",
    "                                               format_type_options=csv_file_format_options)\n",
    "        stage_table_names.append(stage_table_name)\n",
    "\n",
    "\n",
    "    if len(schema2_files) > 0:\n",
    "        load_schema2 = asdfasdf\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "\n",
    "        stage_table_name = load_table_name + str('schema2')\n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                              .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                              .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                              .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                              .option(\"NULL_IF\", \"NULL\")\\\n",
    "                              .schema(load_schema2)\\\n",
    "                              .csv(\"@\"+load_stage_name)\\\n",
    "                              .copy_into_table(stage_table_name, \n",
    "                                               files=schema2_files, \n",
    "                                               format_type_options=csv_file_format_options)\n",
    "        stage_table_names.append(stage_table_name)\n",
    "        \n",
    "    return list(set(stage_table_names))\n",
    "    \n",
    "def transform_trips(session, stage_table_names:list, trips_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "        \n",
    "    #Change all dates to YYYY-MM-DD HH:MI:SS format\n",
    "    date_format_match = \"^([0-9]?[0-9])/([0-9]?[0-9])/([0-9][0-9][0-9][0-9]) ([0-9]?[0-9]):([0-9][0-9])(:[0-9][0-9])?.*$\"\n",
    "    date_format_repl = \"\\\\3-\\\\1-\\\\2 \\\\4:\\\\5\\\\6\"\n",
    "    \n",
    "    for stage_table_name in stage_table_names:\n",
    "        \n",
    "        transdf = session.table(stage_table_name)\n",
    "        transdf.withColumn('STARTTIME', F.regexp_replace(F.col('STARTTIME'),\n",
    "                                                F.lit(date_format_match), \n",
    "                                                F.lit(date_format_repl)))\\\n",
    "               .withColumn('STARTTIME', F.to_timestamp('STARTTIME'))\\\n",
    "               .withColumn('STOPTIME', F.regexp_replace(F.col('STOPTIME'),\n",
    "                                                F.lit(date_format_match), \n",
    "                                                F.lit(date_format_repl)))\\\n",
    "               .withColumn('STOPTIME', F.to_timestamp('STOPTIME'))\\\n",
    "               .select(F.col('STARTTIME'), \n",
    "                       F.col('STOPTIME'), \n",
    "                       F.col('START_STATION_ID'), \n",
    "                       F.col('START_STATION_NAME'), \n",
    "                       F.col('START_STATION_LATITUDE'), \n",
    "                       F.col('START_STATION_LONGITUDE'), \n",
    "                       F.col('END_STATION_ID'), \n",
    "                       F.col('END_STATION_NAME'), F.col('END_STATION_LATITUDE'), \n",
    "                       F.col('END_STATION_LONGITUDE'), \n",
    "                       F.col('USERTYPE'))\\\n",
    "               .write.saveAsTable(trips_table_name)\n",
    "\n",
    "    return trips_table_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
