{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Ingest Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental and Bulk Extract, Load and Transform\n",
    "We expect to get new data every month which we will incrementally load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags import elt as ELT\n",
    "\n",
    "import snowflake.snowpark as snp\n",
    "from datetime import datetime\n",
    "import uuid \n",
    "\n",
    "start = datetime.now()\n",
    "print(\"Start Time =\", start.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "load_stage_name=state_dict['load_stage_name']\n",
    "session.sql('CREATE STAGE IF NOT EXISTS '+load_stage_name).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_base_url=state_dict['download_base_url']\n",
    "load_table_name=state_dict['load_table_name']\n",
    "trips_table_name=state_dict['trips_table_name']\n",
    "\n",
    "file_name_end2 = '202102-citibike-tripdata.csv.zip'\n",
    "file_name_end1 = '201402-citibike-tripdata.zip'\n",
    "file_name_end3 = '202003-citibike-tripdata.csv.zip'\n",
    "\n",
    "files_to_download = [file_name_end1, file_name_end3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "load_stage_names, files_to_load = ELT.extract_trips_to_stage(session=session, \n",
    "                                                            files_to_download=files_to_download, \n",
    "                                                            download_base_url=state_dict['download_base_url'], \n",
    "                                                            load_stage_name=state_dict['load_stage_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stage_table_names = ELT.load_trips_to_raw(session=session, \n",
    "                                          files_to_load=files_to_load, \n",
    "                                          load_stage_names=load_stage_names, \n",
    "                                          load_table_name=state_dict['load_table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trips_table_name = ELT.transform_trips(session=session, \n",
    "                                       stage_table_names=stage_table_names, \n",
    "                                       trips_table_name=state_dict['trips_table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "files_to_ingest=['202004-citibike-tripdata.csv.zip', '202102-citibike-tripdata.csv.zip']\n",
    "schema1_download_files = list()\n",
    "schema2_download_files = list()\n",
    "schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "\n",
    "for file_name in files_to_ingest:\n",
    "    file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "    if file_start_date < schema2_start_date:\n",
    "        schema1_download_files.append(file_name.replace('.zip','.gz'))\n",
    "    else:\n",
    "        schema2_download_files.append(file_name.replace('.zip','.gz'))\n",
    "        \n",
    "files_to_load = {'schema1': schema1_download_files, 'schema2': schema2_download_files}\n",
    "files_to_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/ingest.py\n",
    "def incremental_elt(session, \n",
    "                    state_dict:dict, \n",
    "                    files_to_ingest:list, \n",
    "                    download_role_ARN='',\n",
    "                    download_base_url='') -> str:\n",
    "    \n",
    "    import dags.elt as ELT\n",
    "    from datetime import datetime\n",
    "\n",
    "    load_stage_name=state_dict['load_stage_name']\n",
    "    load_table_name=state_dict['load_table_name']\n",
    "    trips_table_name=state_dict['trips_table_name']\n",
    "    \n",
    "    if download_role_ARN and download_base_url:\n",
    "        print(\"Skipping extract.  Using provided bucket.\")\n",
    "        sql_cmd = 'CREATE OR REPLACE TEMPORARY STAGE TEMP_LOAD_STAGE'\\\n",
    "                  ' url='+str(download_base_url)+\\\n",
    "                  \" credentials=(aws_role='\" + str(download_role_ARN)+\"')\"\n",
    "        session.sql(sql_cmd).collect()\n",
    "        \n",
    "        schema1_download_files = list()\n",
    "        schema2_download_files = list()\n",
    "        schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "\n",
    "        for file_name in files_to_ingest:\n",
    "            file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "            if file_start_date < schema2_start_date:\n",
    "                schema1_download_files.append(file_name.replace('.zip','.gz'))\n",
    "            else:\n",
    "                schema2_download_files.append(file_name.replace('.zip','.gz'))\n",
    "        \n",
    "        \n",
    "        load_stage_names = {'schema1':'TEMP_LOAD_STAGE', 'schema2':'TEMP_LOAD_STAGE'}\n",
    "        files_to_load = {'schema1': schema1_download_files, 'schema2': schema2_download_files}\n",
    "        #print(files_to_load)\n",
    "    else:\n",
    "        print(\"Extracting files from public location.\")\n",
    "        download_base_url=state_dict['download_base_url']\n",
    "        #_ = session.sql('CREATE OR REPLACE TEMPORARY STAGE '+str(load_stage_name)).collect()        \n",
    "        load_stage_names, files_to_load = ELT.extract_trips_to_stage(session=session, \n",
    "                                                                    files_to_download=files_to_ingest, \n",
    "                                                                    download_base_url=download_base_url, \n",
    "                                                                    load_stage_name=load_stage_name)\n",
    "\n",
    "    print(\"Loading files to raw.\")\n",
    "    stage_table_names = ELT.load_trips_to_raw(session=session, \n",
    "                                              files_to_load=files_to_load, \n",
    "                                              load_stage_names=load_stage_names, \n",
    "                                              load_table_name=load_table_name)    \n",
    "    \n",
    "    print(\"Transforming records to trips table.\")\n",
    "    trips_table_name = ELT.transform_trips(session=session, \n",
    "                                           stage_table_names=stage_table_names, \n",
    "                                           trips_table_name=trips_table_name)\n",
    "    return trips_table_name\n",
    "\n",
    "def bulk_elt(session, \n",
    "             state_dict:dict,\n",
    "             download_role_ARN='', \n",
    "             download_base_url=''\n",
    "            ) -> str:\n",
    "    \n",
    "    import dags.elt as ELT\n",
    "    from dags.ingest import incremental_elt\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    #Create a list of filenames to download based on date range\n",
    "    #For files like 201306-citibike-tripdata.zip\n",
    "    date_range1 = pd.period_range(start=datetime.strptime(\"201306\", \"%Y%m\"), \n",
    "                                 end=datetime.strptime(\"201612\", \"%Y%m\"), \n",
    "                                 freq='M').strftime(\"%Y%m\")\n",
    "    file_name_end1 = '-citibike-tripdata.zip'\n",
    "    files_to_extract = [date+file_name_end1 for date in date_range1.to_list()]\n",
    "\n",
    "    #For files like 201701-citibike-tripdata.csv.zip\n",
    "    date_range2 = pd.period_range(start=datetime.strptime(\"201701\", \"%Y%m\"), \n",
    "                                 end=datetime.strptime(\"202002\", \"%Y%m\"), \n",
    "                                 freq='M').strftime(\"%Y%m\")\n",
    "    file_name_end2 = '-citibike-tripdata.csv.zip'\n",
    "    files_to_extract = files_to_extract + [date+file_name_end2 for date in date_range2.to_list()]\n",
    "\n",
    "    if download_role_ARN and download_base_url:\n",
    "        trips_table_name = incremental_elt(session=session, \n",
    "                                           state_dict=state_dict, \n",
    "                                           files_to_ingest=files_to_extract, \n",
    "                                           download_role_ARN=download_role_ARN,\n",
    "                                          download_base_url=download_base_url)\n",
    "    else:\n",
    "        trips_table_name = incremental_elt(session=session, \n",
    "                                           state_dict=state_dict, \n",
    "                                           files_to_ingest=files_to_extract)\n",
    "    \n",
    "    return trips_table_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect('./include/state.json')\n",
    "session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dags.ingest import incremental_elt, bulk_elt\n",
    "incremental_elt(session=session, \n",
    "                state_dict=state_dict, \n",
    "                files_to_ingest=['202003-citibike-tripdata.csv.zip']\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dags.ingest import incremental_elt, bulk_elt\n",
    "incremental_elt(session, state_dict, files_to_ingest=['202004-citibike-tripdata.csv.zip', \n",
    "                                                      '202102-citibike-tripdata.csv.zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dags.ingest import incremental_elt, bulk_elt\n",
    "incremental_elt(session=session, \n",
    "                state_dict=state_dict, \n",
    "                files_to_ingest=['202001-citibike-tripdata.csv.zip', '202005-citibike-tripdata.csv.zip'],\n",
    "                download_role_ARN='arn:aws:iam::484577546576:role/citibike-demo-ml-s3-role',\n",
    "                download_base_url='s3://citibike-demo-ml/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from dags.ingest import incremental_elt, bulk_elt\n",
    "# _ = session.sql('CREATE OR REPLACE DATABASE '+state_dict['connection_parameters']['database']).collect()\n",
    "# _ = session.sql('CREATE SCHEMA '+state_dict['connection_parameters']['schema']).collect() \n",
    "\n",
    "# session.use_warehouse(state_dict['compute_parameters']['fe_warehouse']).collect()\n",
    "\n",
    "# bulk_elt(session=session, state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dags.ingest import incremental_elt, bulk_elt\n",
    "from dags.elt import schema1_definition, schema2_definition\n",
    "\n",
    "_ = session.sql('CREATE OR REPLACE DATABASE '+state_dict['connection_parameters']['database']).collect()\n",
    "_ = session.sql('CREATE SCHEMA '+state_dict['connection_parameters']['schema']).collect() \n",
    "\n",
    "load_schema1 = schema1_definition()\n",
    "session.createDataFrame([[None]*len(load_schema1.names)], schema=load_schema1)\\\n",
    "       .na.drop()\\\n",
    "       .write\\\n",
    "       .saveAsTable(state_dict['load_table_name']+'schema1')\n",
    "\n",
    "load_schema2 = schema2_definition()\n",
    "session.createDataFrame([[None]*len(load_schema2.names)], schema=load_schema2)\\\n",
    "       .na.drop()\\\n",
    "       .write\\\n",
    "       .saveAsTable(state_dict['load_table_name']+'schema2')\n",
    "\n",
    "session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "bulk_elt(session=session, \n",
    "         state_dict=state_dict, \n",
    "         download_role_ARN='arn:aws:iam::484577546576:role/citibike-demo-ml-s3-role',\n",
    "         download_base_url='s3://citibike-demo-ml/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
