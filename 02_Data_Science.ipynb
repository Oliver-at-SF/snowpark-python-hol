{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Modeling\n",
    "\n",
    "Now we're to the fun part - the Data Science. Now that the data engineers have cleaned and loaded the data to the `trips` table, we can begin our model development. For this, we will leverage Snowpark to do the **feature preparation and exploratory analysis**.   This dataset is initially ~100 million rows and is likely too large to fit into memory on our local machine or even a reasonable sized single VM in the cloud. The Snowpark Python client-side Dataframe API \n",
    "allows us to push-down most of the computation for preparation and feature engineering to Snowpark. For security and goverance reasons we can read data into memory for model training and inference but no intermediate data products can be stored outside of Snowflake.  \n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The data scientist can develop in their tool of choice (ie. DataRobot, Dataiku, H2o, Sagemaker, AzureML, etc.).  Snowpark Python makes it possible to use any environment with a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the data scientist may use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Data in `trips` table.   \n",
    "Output: Feature engineering logic.  Train function.  Predict function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo we will rewind in time and assume that it is March 1, 2020.  With the bulk ingest we have 94M records from 2013 to March 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#today = '2020-03-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q matplotlib seaborn pytorch-tabnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "import pandas as pd\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.WARN)\n",
    "# logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_table_name = state_dict['trips_table_name']\n",
    "holiday_table_name = 'HOLIDAYS'\n",
    "precip_table_name = 'WEATHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(trips_table_name).select(F.min('STARTTIME'), F.max('STARTTIME')).show()\n",
    "session.table(trips_table_name).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too large to fit in memory on my local system.  Lets summarize trips to daily resolution and inspect the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf = session.table(trips_table_name)\n",
    "snowdf.with_column('DATE', F.to_date('STARTTIME')).group_by('DATE').count().sort('DATE').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we aggregate the data at the day level we have a small enough dataset to fit in memory.  But we may want to provide a more granular time series (ie. hour or minute-level) or perhaps our data will grow considerably over time.  In either case we can't rely on in-memory computation and will want to push-down as much computation as possible to Snowflake.  \n",
    "  \n",
    "For exploration purposes we can see a good daily and annual seasonality in the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = snowdf.with_column(\"date\", F.to_date(\"STARTTIME\")).group_by(\"date\").count().sort(\"date\").to_pandas()\n",
    "df.head()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.lineplot(x='DATE', y='COUNT', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may not be able to get a good model that can predict across ALL stations.  Lets start with just the busiest stations(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf.filter(F.col('START_STATION_ID').is_not_null()) \\\n",
    "      .group_by('START_STATION_ID') \\\n",
    "      .count() \\\n",
    "      .sort('COUNT', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we will build with the busiest station \"Central Park S & 6 Ave\" which is STATION_ID=519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stations = snowdf.filter(F.col('START_STATION_ID').is_not_null()) \\\n",
    "                                                   .groupBy('START_STATION_ID') \\\n",
    "                                                   .count() \\\n",
    "                                                   .sort('COUNT', ascending=False) \\\n",
    "                                                   .toPandas()['START_STATION_ID'].values.tolist()\n",
    "top_stations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = snowdf.filter(F.col('START_STATION_ID') == top_stations[0]) \\\n",
    "      .withColumn('DATE', \n",
    "                  F.call_builtin('DATE_TRUNC', ('DAY', F.col('STARTTIME')))) \\\n",
    "      .groupBy('DATE') \\\n",
    "      .count() \\\n",
    "      .sort('DATE').toPandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.lineplot(x='DATE', y='COUNT', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start building the features for a regression model we might start with daily, weekly, monthly, quarterly lag features. \n",
    "  \n",
    "Snowpark has many [functions](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/_autosummary/snowflake.snowpark.functions.html) for things like transformation, statistical analysis, etc. We will use the `lag()` window function to generate lag features.  \n",
    "  \n",
    "In addition to the rich set of Snowpark functions, users can also tap into the wealth of [Snowflake built-in functions](https://docs.snowflake.com/en/sql-reference/functions-all.html) using the `call_builtin()` function. \n",
    "\n",
    "Here we will generate our features in a function which we may need to add to over time.  In the end the function provides a running list of transformations to build our feature pipeline for inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(snowdf):\n",
    "    \n",
    "    snowdf = snowdf.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                           F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                   .group_by(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                   .count()\n",
    "    \n",
    "    #Impute missing values for lag columns using mean of the previous period.\n",
    "    mean_1 = round(snowdf.sort('DATE').limit(1).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_7 = round(snowdf.sort('DATE').limit(7).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_30 = round(snowdf.sort('DATE').limit(30).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_365 = round(snowdf.sort('DATE').limit(365).select(F.mean('COUNT')).collect()[0][0])\n",
    "\n",
    "    date_win = snp.Window.order_by('DATE')\n",
    "\n",
    "    snowdf = snowdf.with_column('LAG_1', F.lag('COUNT', offset=1, default_value=mean_1) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .with_column('LAG_7', F.lag('COUNT', offset=7, default_value=mean_7) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .with_column('LAG_30', F.lag('COUNT', offset=30, default_value=mean_30) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .with_column('LAG_365', F.lag('COUNT', offset=365, default_value=mean_365) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .na.drop()\n",
    "    return snowdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf = generate_features(snowdf.filter(F.col('START_STATION_ID') == top_stations[0]))\n",
    "train_snowdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Development and Experimentation\n",
    "We will use [pytorch_tabnet](https://github.com/dreamquark-ai/tabnet) from the 2019 [tabnet paper](https://arxiv.org/pdf/1908.07442.pdf) by Arik, S. O. and Pfister, T.  Tabnet is a powerful deep learning framework for attentive, interpretable learning on tabular data.  Rather than substantial focus on hyper-parameter optimization  we will start with an initial set of hyper-parameters and focus on iterating over input features for now.  \n",
    "  \n",
    "Rather than a random split of training/validation data we will split the training dataset using a `cutpoint`.  For example we will save the final 365 days of the dataset as validation and train with the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, cutpoint=365, cat_idxs=[], cat_dims=[]):    \n",
    "    X_valid = X[-cutpoint:]\n",
    "    y_valid = y[-cutpoint:]\n",
    "    X_train = X[:-cutpoint]\n",
    "    y_train = y[:-cutpoint]\n",
    "\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    import pandas as pd\n",
    "\n",
    "    max_epochs = 100\n",
    "    \n",
    "    batch_df = pd.DataFrame(range(2,65,2), columns=['batch_size'])\n",
    "    batch_df['batch_remainder'] = len(X_train)%batch_df['batch_size']\n",
    "    optimal_batch_size=int(batch_df['batch_size'].where(batch_df['batch_remainder']==batch_df['batch_remainder'].min()).max())\n",
    "    \n",
    "    print('Selected batch size '+str(optimal_batch_size)+' for input data size: '+str(len(X_train)))\n",
    "    \n",
    "    regression_model = TabNetRegressor(cat_idxs=cat_idxs, cat_dims=cat_dims)\n",
    "\n",
    "    regression_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=10,\n",
    "        batch_size=optimal_batch_size, \n",
    "        virtual_batch_size=optimal_batch_size/2,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "    \n",
    "    return regression_model\n",
    "\n",
    "def predict(model, X):\n",
    "    y_hat = model.predict(X).reshape(-1)\n",
    "    return y_hat\n",
    "    \n",
    "def plot(df, x_lab:str, y_true_lab:str, y_pred_lab:str):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    df = pd.melt(df, id_vars=[x_lab], value_vars=[y_true_lab, y_pred_lab])\n",
    "    ax = sns.lineplot(x=x_lab, y='value', hue='variable', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a pandas dataframe using toPandas() which will generate the features as a pyarrow dataset and efficiently read them into memory in pandas locally.  \n",
    "  \n",
    "Let's train our first model to get a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_predict(df, cat_idxs=[], cat_dims=[]):\n",
    "    target = ['COUNT']\n",
    "    feature_columns = [feature.replace('\\\"', '') for feature in df.columns]\n",
    "    feature_columns.remove(target[0])\n",
    "    feature_columns.remove('DATE')\n",
    "    feature_columns.remove('STATION_ID')\n",
    "\n",
    "    model = train(df[feature_columns].astype(float).values, \n",
    "                  df[target].values,\n",
    "                  cat_idxs=cat_idxs, \n",
    "                  cat_dims=cat_dims)\n",
    "    df['Y_PRED'] = predict(model, df[feature_columns].astype(float).values).astype('int')\n",
    "\n",
    "    explain_df = pd.DataFrame(model.explain(df[feature_columns].astype(float).values)[0], \n",
    "                          columns = feature_columns).add_prefix('EXPL_').round(2)\n",
    "    df = pd.concat([df.set_index('DATE').reset_index(), explain_df], axis=1)\n",
    "    \n",
    "    MSE = mean_squared_error(y_pred=df['Y_PRED'], y_true=df[target])\n",
    "    display(\"Error for training dataset is: \"+str(MSE))\n",
    "    \n",
    "    return df, model, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = train_snowdf.sort('DATE', ascending=True).to_pandas()\n",
    "df, model, feature_columns = train_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we add US holidays as a feature?\n",
    "Pandas has great support for holiday lists.  We can generate a pandas dataframe and then upload it as a table to create a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holiday_df(session, start_date, end_date):\n",
    "    from snowflake.snowpark import functions as F \n",
    "    import pandas as pd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holiday_list = pd.DataFrame(cal.holidays(start=start_date, end=end_date), columns=['DATE'], dtype=str)\n",
    "    holiday_df = session.create_dataframe(holiday_list) \\\n",
    "                        .toDF(holiday_list.columns[0]) \\\n",
    "                        .with_column(\"HOLIDAY\", F.lit(1))\n",
    "    return holiday_df\n",
    "    \n",
    "start_date, end_date = train_snowdf.select(F.min('DATE'), F.max('DATE')).collect()[0][0:2]\n",
    "\n",
    "holiday_df = generate_holiday_df(session, start_date=start_date, end_date=end_date)\n",
    "holiday_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an in-memory instance of the holiday dataframe.  In production we probably want to materialize this feature as a table or view.  We'll see later how that is easy to do but for now we have a function to generate it and join it to our training dataframe as a one-hot feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we join there is a possibility of losing data from our feature set if the join column doesn't cover the same dates.  So its good to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf = train_snowdf.join(holiday_df, 'DATE', join_type='left') \\\n",
    "                           .na.fill({'HOLIDAY':0}) \\\n",
    "                           .sort('DATE', ascending=True)\n",
    "train_snowdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we train we need to specify this new holiday feature as a categorical feature.  Its already encoded (by definition) so nothing to add there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = train_snowdf.sort('DATE', ascending=True).to_pandas()\n",
    "df, model, feature_columns = train_predict(df, cat_idxs=[-1], cat_dims=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhancing Model Accuracy With Weather Data\n",
    "\n",
    "Let's see how we can make our model even better with weather data. Its likely that weather and the amount of precipitation on the day will be an important signal for our model. \n",
    "  \n",
    "We might start by downloading weather data or setting up an API.  But this is a lot of work when we want to build inference pipelines later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_precip_df(session, start_date, end_date):\n",
    "    from snowflake.snowpark import functions as F \n",
    "    import pandas as pd\n",
    "\n",
    "    tempdf = pd.read_csv('./include/weather.csv')\n",
    "    tempdf['DATE']=pd.to_datetime(tempdf['dt_iso'].str.replace(' UTC', ''), \n",
    "                                 format='%Y-%m-%d %H:%M:%S %z', \n",
    "                                 utc=True).dt.tz_convert('America/New_York').dt.date\n",
    "    tempdf = tempdf.groupby('DATE', as_index=False).agg(PRECIP=('rain_1h', 'mean')).round()\n",
    "\n",
    "    precip_df = session.create_dataframe(tempdf).filter((F.col('DATE') >= start_date) &\n",
    "                                                        (F.col('DATE') <= end_date))\\\n",
    "                       .fillna({'PRECIP':0})\n",
    "    return precip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Optionally use the [Snowflake Data Marketplace](https://www.snowflake.com/data-marketplace/?_sft_dataset-category=weather) & Data Sharing\n",
    "The Snowflake marketplace would allow us to very easily access weather data and join it with the trips data.  Not only is this faster and more scalable than building from a csv file or API, it is extremely easy to setup an operational pipeline to keep it fresh.  \n",
    "  \n",
    "For this demo we will continue to use the csv format as we cannot be sure that each user has access to the marketplace data already setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def generate_precip_df(start_date, end_date):\n",
    "#     precip_df = session.table('CITIBIKE_WEATHER_V3.WEATHER_V3.HISTORY_DAY') \\\n",
    "#                          .filter(F.col('POSTAL_CODE') == '08562')\\\n",
    "#                          .select(F.col('DATE_VALID_STD').alias('DATE'), \n",
    "#                                  F.col('TOT_PRECIPITATION_IN').alias('PRECIP'))\n",
    "#     #precip_df = session.table('weather')\n",
    "\n",
    "#     return precip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a precip feature by joining the precipitation dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = train_snowdf.select(F.min('DATE'), F.max('DATE')).collect()[0][0:2]\n",
    "\n",
    "precip_df = generate_precip_df(session, start_date=start_date, end_date=end_date)\n",
    "\n",
    "train_snowdf = train_snowdf.join(precip_df, 'DATE', 'inner').sort('DATE', ascending=True)\n",
    "\n",
    "train_snowdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check the date range after the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf.select(F.min('DATE'), F.max('DATE')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = train_snowdf.sort('DATE', ascending=True).to_pandas()\n",
    "df, model, feature_columns = train_predict(df, cat_idxs=[-2], cat_dims=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at the feature importance\n",
    "One reason we chose tabnet for this analysis is the built-in abilities for explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([model.feature_importances_], columns=feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets consolidate and upate our feature functions and save them for the ML engineering team to operationalize.\n",
    "Based on the feature importance above we will go with 1 and 7 day lags only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/feature_engineering.py\n",
    "\n",
    "def generate_holiday_df(session, start_date, end_date):\n",
    "    from snowflake.snowpark import functions as F \n",
    "    import pandas as pd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holiday_list = pd.DataFrame(cal.holidays(start=start_date, end=end_date), columns=['DATE'], dtype=str)\n",
    "    holiday_df = session.create_dataframe(holiday_list) \\\n",
    "                        .toDF(holiday_list.columns[0]) \\\n",
    "                        .with_column(\"HOLIDAY\", F.lit(1))\n",
    "    return holiday_df\n",
    "\n",
    "def generate_precip_df(session, start_date, end_date):\n",
    "    from snowflake.snowpark import functions as F \n",
    "    import pandas as pd\n",
    "\n",
    "    tempdf = pd.read_csv('./include/weather.csv')\n",
    "    tempdf['DATE']=pd.to_datetime(tempdf['dt_iso'].str.replace(' UTC', ''), \n",
    "                                 format='%Y-%m-%d %H:%M:%S %z', \n",
    "                                 utc=True).dt.tz_convert('America/New_York').dt.date\n",
    "    tempdf = tempdf.groupby('DATE', as_index=False).agg(PRECIP=('rain_1h', 'mean')).round()\n",
    "\n",
    "    precip_df = session.create_dataframe(tempdf).filter((F.col('DATE') >= start_date) &\n",
    "                                                        (F.col('DATE') <= end_date))\\\n",
    "                       .fillna({'PRECIP':0})\n",
    "    return precip_df\n",
    "\n",
    "def generate_features(session, input_df, holiday_table_name, precip_table_name):\n",
    "    import snowflake.snowpark as snp\n",
    "    from snowflake.snowpark import functions as F \n",
    "    \n",
    "    start_date, end_date = input_df.select(F.min('STARTTIME'), F.max('STARTTIME')).collect()[0][0:2]\n",
    "\n",
    "    holiday_df = session.table(holiday_table_name)\n",
    "    try: \n",
    "        _ = holiday_df.columns()\n",
    "    except:\n",
    "        holiday_df = generate_holiday_df(session=session,\n",
    "                                         start_date=start_date,\n",
    "                                         end_date=end_date)\n",
    "        \n",
    "    precip_df = session.table(precip_table_name)\n",
    "    try: \n",
    "        _ = precip_df.columns()\n",
    "    except:\n",
    "        precip_df = generate_precip_df(session=session,\n",
    "                                       start_date=start_date,\n",
    "                                       end_date=end_date)\n",
    "\n",
    "    feature_df = input_df.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                                 F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                         .replace({'NULL': None}, subset=['STATION_ID'])\\\n",
    "                         .groupBy(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                         .count()\n",
    "\n",
    "    #Impute missing values for lag columns using mean of the previous period.\n",
    "    mean_1 = round(feature_df.sort('DATE').limit(1).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_7 = round(feature_df.sort('DATE').limit(7).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_365 = round(feature_df.sort('DATE').limit(365).select(F.mean('COUNT')).collect()[0][0])\n",
    "\n",
    "    date_win = snp.Window.orderBy('DATE')\n",
    "\n",
    "    feature_df = feature_df.with_column('LAG_1', F.lag('COUNT', offset=1, default_value=mean_1) \\\n",
    "                                         .over(date_win)) \\\n",
    "                           .with_column('LAG_7', F.lag('COUNT', offset=7, default_value=mean_7) \\\n",
    "                                         .over(date_win)) \\\n",
    "                           .with_column('LAG_365', F.lag('COUNT', offset=365, default_value=mean_365) \\\n",
    "                                         .over(date_win)) \\\n",
    "                           .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0}) \\\n",
    "                           .join(precip_df, 'DATE', 'inner') \\\n",
    "                          .na.drop() \n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make one final test of feature engineering with our new code then train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dags.feature_engineering import generate_features\n",
    "\n",
    "snowdf = session.table(trips_table_name).filter(F.col('START_STATION_ID') == top_stations[0])\n",
    "\n",
    "train_snowdf = generate_features(session=session, \n",
    "                                 input_df=snowdf, \n",
    "                                 holiday_table_name=holiday_table_name, \n",
    "                                 precip_table_name=precip_table_name)\n",
    "\n",
    "df = train_snowdf.sort('DATE', ascending=True).toPandas()\n",
    "df, model, feature_columns = train_predict(df, cat_idxs=[-2], cat_dims=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([model.feature_importances_], columns=feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Testing and Evaluation\n",
    "We will want to be able to look back over an period of time to check model performance over a specific period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_past_df(session, trips_table_name, station_id, pred_period, holiday_table_name, precip_table_name):\n",
    "    from dags.feature_engineering import generate_features\n",
    "    \n",
    "    end_date = session.table(trips_table_name).select(F.max('STARTTIME')).collect()[0][0]\n",
    "    pastdf = session.table(trips_table_name)\\\n",
    "                    .filter(F.col('START_STATION_ID') == station_id)\\\n",
    "                    .filter(F.to_date('STARTTIME') >= F.dateadd('DAY', \n",
    "                                                                F.lit(-7+pred_period), \n",
    "                                                                F.lit(end_date)))\n",
    "    newdf = generate_features(session=session, \n",
    "                              input_df=pastdf, \n",
    "                              holiday_table_name=holiday_table_name,\n",
    "                              precip_table_name=precip_table_name)\n",
    "\n",
    "    newdf = newdf.filter(F.to_date('DATE') >= F.dateadd(pred_interval,\n",
    "                                                        F.lit(pred_period),\n",
    "                                                        F.lit(end_date)))\n",
    "  \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our model performed for the previous 90 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_period=-90\n",
    "pred_interval='DAY'\n",
    "past_df = generate_past_df(session=session, \n",
    "                           trips_table_name=trips_table_name, \n",
    "                           station_id=top_stations[0], \n",
    "                           pred_period=pred_period, \n",
    "                           holiday_table_name=holiday_table_name,\n",
    "                           precip_table_name=precip_table_name)\n",
    "\n",
    "target = ['COUNT']\n",
    "\n",
    "df = past_df.sort('DATE', ascending=True).toPandas()\n",
    "\n",
    "df['Y_PRED'] = predict(model, df[feature_columns].astype(float).values).astype('int')\n",
    "\n",
    "# MSE = mean_squared_error(y_pred=df['Y_PRED'], y_true=df[target])\n",
    "# display(\"Error for \"+str(prediction_period)+\" days from \"+today+\" is: \"+str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_df.select(F.min('DATE'), F.max('DATE')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
