{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Ops\n",
    "In this section of the demo, we will utilize Snowpark's Python client-side Dataframe API as well as the Snowpark server-side runtime to create an **ML ops pipeline**.  We will take the functions created by the ML Engineer and create a set of functions that can be easily automated with the company's orchestration tools. \n",
    "\n",
    "The ML Engineer must create a pipeline to **automate deployment** of models and batch predictions where the business users can consume them easily from dashboards and analytics tools like Tableau or Power BI.  Predictions will be made for the top 10 busiest stations.  The predictions must be accompanied by an explanation of which features were most impactful for the prediction.  \n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The ML engineer can develop in their tool of choice (ie. VS Code, IntelliJ, Pycharm, Eclipse, etc.).  Snowpark Python makes it possible to use any environment where they have a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the ML engineer can use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Data in `trips` table.  Feature engineering, train, predict functions from data scientist.  \n",
    "Output: Automatable pipeline of feature engineering, train, predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dag/snowpark_connection.py\n",
    "\n",
    "def snowpark_connect(creds_file='creds.json'):\n",
    "    import snowflake.snowpark as snp\n",
    "    import os, json \n",
    "    \n",
    "    with open(os.path.join(creds_file)) as f:\n",
    "        data = json.load(f)\n",
    "        connection_parameters = {\n",
    "            'account': data['account'],\n",
    "            'user': data['username'],\n",
    "            'password': data['password'],\n",
    "            'role': data['role'],\n",
    "            'warehouse': data['task_warehouse'],\n",
    "            'database': data['database'],\n",
    "            'schema': data['schema']\n",
    "        }\n",
    "        compute_parameters = {\n",
    "            'load_warehouse': data['load_warehouse'],\n",
    "            'fe_warehouse': data['fe_warehouse'],\n",
    "            'train_warehouse': data['train_warehouse'],\n",
    "        }\n",
    "    \n",
    "    session = snp.Session.builder.configs(connection_parameters).create()\n",
    "    return session, compute_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dag.snowpark_connection import snowpark_connect\n",
    "session, compute_parameters = snowpark_connect('creds.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions as F\n",
    "import uuid\n",
    "state_dict = {\n",
    "        \"trips_table_name\":\"TRIPS\",\n",
    "        \"load_stage_name\":\"LOAD_STAGE\",\n",
    "        \"model_stage_name\":\"MODEL_STAGE\",\n",
    "        \"model_id\": str(uuid.uuid1()).replace('-', '_')\n",
    "    }\n",
    "\n",
    "start_date, end_date = session.table(state_dict['trips_table_name']) \\\n",
    "                              .select(F.min('STARTTIME'), F.max('STARTTIME')).collect()[0][0:2]\n",
    "state_dict.update({\"start_date\":start_date})\n",
    "state_dict.update({\"end_date\":end_date})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business doesn't actively maintain bicycle stock at EVERY station.  We only need predictions for the `top_n` number of stations.  Initially that is 10 but it might change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy the model training and inference as a permanent [Python Snowpark User-Defined Function (UDF)](https://docs.snowflake.com/en/LIMITEDACCESS/snowpark-python.html#creating-user-defined-functions-udfs-for-dataframes). This will make the function available to not only our automated training/inference pipeline but also to any users needing the function for manually generated predictions.  \n",
    "  \n",
    "As a permanent function we will need a staging area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = session.sql('CREATE STAGE IF NOT EXISTS ' + state_dict['model_stage_name']).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For production we need to be able to reproduce results.  The `trips` table will change as new data is loaded each month so we need a point-in-time snapshot.  Snowflake [Zero-Copy Cloning](https://docs.snowflake.com/en/sql-reference/sql/create-clone.html) allows us to do this with copy-on-write features so we don't have multiple copies of the same data.  We will create a unique ID to identify each training/inference run as well as the features and predictions generated.  We can use [object tagging](https://docs.snowflake.com/en/user-guide/object-tagging.html) to tag each object with the `model_id` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_table_name = 'TRIPS_CLONE_'+state_dict[\"model_id\"]\n",
    "state_dict.update({\"clone_table_name\":clone_table_name})\n",
    "\n",
    "_ = session.sql('CREATE OR REPLACE TABLE '+clone_table_name+\" CLONE \"+state_dict[\"trips_table_name\"]).collect()\n",
    "_ = session.sql('CREATE TAG IF NOT EXISTS model_id_tag').collect()\n",
    "_ = session.sql(\"ALTER TABLE \"+clone_table_name+\" SET TAG model_id_tag = '\"+state_dict[\"model_id\"]+\"'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing the functions created by the ML Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dag.mlops_pipeline import deploy_pred_train_udf\n",
    "from dag.mlops_pipeline import materialize_holiday_table\n",
    "from dag.mlops_pipeline import materialize_precip_table\n",
    "#from dag.mlops_pipeline import generate_feature_views\n",
    "#from dag.mlops_pipeline import train_predict_feature_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will be orchestrated by our companies orchestration framework but we will test the steps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'station_train_predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_udf_name \u001b[38;5;241m=\u001b[39m \u001b[43mdeploy_pred_train_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmodel_stage_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_stage_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m state_dict\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_udf_name\u001b[39m\u001b[38;5;124m\"\u001b[39m:model_udf_name})\n",
      "File \u001b[0;32m~/Documents/code/snowpark-python/citibike-ml-tko-HOL/dag/mlops_pipeline.py:23\u001b[0m, in \u001b[0;36mdeploy_pred_train_udf\u001b[0;34m(session, model_stage_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeploy_pred_train_udf\u001b[39m(session, model_stage_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstation_train_predict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m station_train_predict_func\n\u001b[1;32m     25\u001b[0m     session\u001b[38;5;241m.\u001b[39mclearImports()\n\u001b[1;32m     26\u001b[0m     session\u001b[38;5;241m.\u001b[39maddImport(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch_tabnet.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'station_train_predict'"
     ]
    }
   ],
   "source": [
    "model_udf_name = deploy_pred_train_udf(session=session, \n",
    "                                       model_stage_name=state_dict['model_stage_name']\n",
    "                                      )\n",
    "                \n",
    "state_dict.update({\"model_udf_name\":model_udf_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_table_name = materialize_holiday_table(session=session,\n",
    "                                               start_date=state_dict['start_date'], \n",
    "                                               end_date=state_dict['end_date'], \n",
    "                                               holiday_table_name='holidays'\n",
    "                                              )\n",
    "        \n",
    "state_dict.update({\"holiday_table_name\":holiday_table_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_table_name = materialize_precip_table(session=session,\n",
    "                                             start_date=state_dict['start_date'], \n",
    "                                             end_date=state_dict['end_date'], \n",
    "                                             precip_table_name='weather'\n",
    "                                             )\n",
    "state_dict.update({\"precip_table_name\":precip_table_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions as F\n",
    "testdf = session.table('CLONE_454A3F10_808F_11EC_A712_ACDE48001122')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_period = 'DAY'\n",
    "date_win = snp.Window.orderBy('DATE')\n",
    "holiday_df = session.table(holiday_table_name)\n",
    "precip_df = session.table(precip_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.select('STARTTIME', 'START_STATION_ID')\\\n",
    "      .withColumn('DATE', F.call_builtin('DATE_TRUNC', (agg_period, F.col('STARTTIME'))))\\\n",
    "      .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "      .join(precip_df, 'DATE', 'inner')\\\n",
    "      .groupBy(F.col('DATE'), F.col('START_STATION_ID'))\\\n",
    "      .count()\\\n",
    "      .filter(F.col('START_STATION_ID') == '519')\\\n",
    "      .sort('DATE')\\\n",
    "      .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.select('STARTTIME', 'START_STATION_ID')\\\n",
    "      .withColumn('DATE', F.call_builtin('DATE_TRUNC', (agg_period, F.col('STARTTIME'))))\\\n",
    "      .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "      .groupBy(F.col('DATE'), F.col('START_STATION_ID'))\\\n",
    "      .count()\\\n",
    "      .sort('DATE')\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "feature_view_names = generate_feature_views(session=session, \n",
    "                                            clone_table_name=clone_table_name, \n",
    "                                            feature_view_name=feature_view_name, \n",
    "                                            holiday_table_name=holiday_table_name, \n",
    "                                            precip_table_name=holiday_table_name,\n",
    "                                            target_column='COUNT', \n",
    "                                            top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "pred_table_name = train_predict_feature_views(session=session, \n",
    "                                               station_train_pred_udf_name=model_udf_name, \n",
    "                                               feature_view_names=feature_view_names, \n",
    "                                               pred_table_name=pred_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(pred_table_name).select('STATION_ID').distinct().count() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile dag/mlops_pipeline.py\n",
    "\n",
    "def materialize_holiday_table(session, start_date, end_date, holiday_table_name) -> str:\n",
    "    from feature_engineering import generate_holiday_df\n",
    "    from datetime import datetime\n",
    "\n",
    "    holiday_df = generate_holiday_df(session=session, start_date=start_date, end_date=datetime.now())\n",
    "    holiday_df.write.mode('overwrite').saveAsTable(holiday_table_name)\n",
    "    \n",
    "    return holiday_table_name\n",
    "\n",
    "def materialize_precip_table(session, start_date, end_date, precip_table_name) -> str:\n",
    "    from feature_engineering import generate_precip_df\n",
    "    from datetime import datetime\n",
    "\n",
    "    precip_df = generate_precip_df(session=session, start_date=start_date, end_date=datetime.now())\n",
    "    precip_df.write.mode('overwrite').saveAsTable(precip_table_name)\n",
    "    \n",
    "    return precip_table_name\n",
    "\n",
    "\n",
    "def deploy_pred_train_udf(session, model_stage_name) -> str:\n",
    "    from station_train_predict import station_train_predict_func\n",
    "\n",
    "    session.clearImports()\n",
    "    session.addImport('pytorch_tabnet.zip')\n",
    "    session.addImport('station_train_predict.py')\n",
    "\n",
    "    station_train_predict_udf = session.udf.register(station_train_predict_func, \n",
    "                                                  name=\"station_train_predict_udf\",\n",
    "                                                  is_permanent=True,\n",
    "                                                  stage_location='@'+str(model_stage_name), \n",
    "                                                  replace=True)\n",
    "    return station_train_predict_udf.name\n",
    "\n",
    "\n",
    "def generate_feature_views(session, \n",
    "                           clone_table_name, \n",
    "                           feature_view_name, \n",
    "                           holiday_table_name, \n",
    "                           precip_table_name, \n",
    "                           target_column, \n",
    "                           top_n) -> list:\n",
    "    from feature_engineering import generate_features\n",
    "    from snowflake.snowpark import functions as F\n",
    "\n",
    "    feature_view_names = list()\n",
    "    \n",
    "    top_n_station_ids = session.table(clone_table_name).filter(F.col('START_STATION_ID').is_not_null()) \\\n",
    "                                                       .groupBy('START_STATION_ID') \\\n",
    "                                                       .count() \\\n",
    "                                                       .sort('COUNT', ascending=False) \\\n",
    "                                                       .limit(top_n) \\\n",
    "                                                       .collect()\n",
    "    top_n_station_ids = [stations['START_STATION_ID'] for stations in top_n_station_ids]\n",
    "\n",
    "    for station in top_n_station_ids:\n",
    "        feature_df = generate_features(session=session, \n",
    "                                       input_df=session.table(clone_table_name)\\\n",
    "                                                       .filter(F.col('START_STATION_ID') == station)\\\n",
    "                                                       .sort('DATE', ascending=True), \n",
    "                                       holiday_table_name=holiday_table_name, \n",
    "                                       precip_table_name=precip_table_name)\n",
    "\n",
    "        input_columns_str = str(' ').join(feature_df.columns).replace('\\\"', \"\")\n",
    "\n",
    "        feature_df = feature_df.select(F.array_agg(F.array_construct(F.col('*'))).alias('input_data'), \n",
    "                                       F.lit(station).alias('station_id'),\n",
    "                                       F.lit(input_columns_str).alias('input_column_names'),\n",
    "                                       F.lit(target_column).alias('target_column'))  \n",
    "\n",
    "        station_feature_view_name = feature_view_name.replace('<station_id>', station)\n",
    "        feature_df.createOrReplaceView(station_feature_view_name)\n",
    "        feature_view_names.append(station_feature_view_name)\n",
    "\n",
    "    return feature_view_names\n",
    "\n",
    "\n",
    "def train_predict_feature_views(session, station_train_pred_udf_name, feature_view_names, pred_table_name) -> str:\n",
    "    from snowflake.snowpark import functions as F\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    \n",
    "    cutpoint=365\n",
    "    max_epochs=1000\n",
    "    \n",
    "    for view in feature_view_names:\n",
    "        feature_df = session.table(view)\n",
    "        output_df = feature_df.select(F.call_udf(station_train_pred_udf_name, \n",
    "                                                 'INPUT_DATA', \n",
    "                                                 'INPUT_COLUMN_NAMES', \n",
    "                                                 'TARGET_COLUMN', \n",
    "                                                 F.lit(cutpoint), \n",
    "                                                 F.lit(max_epochs))).collect()\n",
    "\n",
    "        df = pd.DataFrame(data = ast.literal_eval(output_df[0][0])[0], \n",
    "                      columns = ast.literal_eval(output_df[0][0])[1])\n",
    "\n",
    "        df['DATE'] = pd.to_datetime(df['DATE']).dt.date\n",
    "        df['STATION_ID'] = feature_df.select('STATION_ID').collect()[0][0]\n",
    "\n",
    "        output_df = session.createDataFrame(df).write.saveAsTable(pred_table_name)\n",
    "    \n",
    "    return pred_table_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
