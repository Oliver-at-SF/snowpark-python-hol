{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Pipeline\n",
    "In this section of the demo, we consolidate all previous steps for a full, end-to-end pipeline for incremental ingest, feature engineering, training, prediction, and evaluation.\n",
    "\n",
    "This will be integrated into **our company's orchestration framework** but showing it all in one place will allow our dev ops team to implement it. \n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The ML engineer can develop in their tool of choice (ie. VS Code, IntelliJ, Pycharm, Eclipse, etc.).  Snowpark Python makes it possible to use any environment where they have a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the ML engineer can use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Set of python functions from the Data Engineer, Data Scientist, and ML Engineer.  \n",
    "Output: N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_download = ['202003-citibike-tripdata.csv.zip']\n",
    "\n",
    "def snowpark_citibike_ml_taskflow(files_to_download:list):\n",
    "    from snowpark_connection import snowpark_connect\n",
    "\n",
    "    import uuid\n",
    "    \n",
    "    state_dict = {\n",
    "        \"download_base_url\":\"https://s3.amazonaws.com/tripdata/\",\n",
    "        \"load_table_name\":\"RAW_\",\n",
    "        \"trips_table_name\":\"TRIPS\",\n",
    "        \"load_stage_name\":\"LOAD_STAGE\",\n",
    "        \"model_stage_name\":\"MODEL_STAGE\",\n",
    "        \"model_id\": str(uuid.uuid1()).replace('-', '_')\n",
    "    }\n",
    "    \n",
    "    def snowpark_database_setup(state_dict:dict)-> dict: \n",
    "        import snowflake.snowpark.functions as F\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        start_date, end_date = session.table(state_dict['trips_table_name']) \\\n",
    "                              .select(F.min('STARTTIME'), F.max('STARTTIME')).collect()[0][0:2]\n",
    "        state_dict.update({\"start_date\":start_date})\n",
    "        state_dict.update({\"end_date\":end_date})\n",
    "        \n",
    "        _ = session.sql('CREATE STAGE IF NOT EXISTS ' + state_dict['model_stage_name']).collect()\n",
    "        _ = session.sql('CREATE STAGE IF NOT EXISTS ' + state_dict['load_stage_name']).collect()\n",
    "        \n",
    "        session.close()\n",
    "\n",
    "        return state_dict\n",
    "    \n",
    "    def  incremental_elt_task(state_dict: dict, files_to_download:list)-> dict:\n",
    "        from ingest import incremental_elt\n",
    "\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        _ = session.sql('USE WAREHOUSE '+compute_parameters['load_warehouse']).collect()\n",
    "\n",
    "        print('Ingesting '+str(files_to_download))\n",
    "        _ = incremental_elt(session=session, \n",
    "                            load_stage_name=state_dict['load_stage_name'], \n",
    "                            files_to_download=files_to_download, \n",
    "                            download_base_url=state_dict['download_base_url'], \n",
    "                            load_table_name=state_dict['load_table_name'], \n",
    "                            trips_table_name=state_dict['trips_table_name']\n",
    "                            )\n",
    "        \n",
    "        session.close()\n",
    "        return state_dict\n",
    "    \n",
    "    def deploy_model_udf_task(state_dict:dict)-> dict:\n",
    "        from mlops_pipeline import deploy_pred_train_udf\n",
    "        print('Deploying station model')\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        model_udf_name = deploy_pred_train_udf(session=session, \n",
    "                                               model_stage_name=state_dict['model_stage_name']\n",
    "                                              )\n",
    "                \n",
    "        state_dict.update({\"model_udf_name\":model_udf_name})\n",
    "\n",
    "        session.close()\n",
    "        return state_dict\n",
    "\n",
    "    def materialize_holiday_task(state_dict: dict)-> dict:\n",
    "        from mlops_pipeline import materialize_holiday_table\n",
    "        print('Materializing holiday table')\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        \n",
    "        holiday_table_name = materialize_holiday_table(session=session,\n",
    "                                                       start_date=state_dict['start_date'], \n",
    "                                                       end_date=state_dict['end_date'], \n",
    "                                                       holiday_table_name='holidays'\n",
    "                                                      )\n",
    "        \n",
    "        state_dict.update({\"holiday_table_name\":holiday_table_name})\n",
    "\n",
    "        session.close()\n",
    "        return state_dict\n",
    "\n",
    "    def materialize_precip_task(state_dict: dict)-> dict:\n",
    "        from mlops_pipeline import materialize_precip_table\n",
    "        print('Materializing weather table')\n",
    "        \n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        \n",
    "        precip_table_name = materialize_precip_table(session=session,\n",
    "                                                     start_date=state_dict['start_date'], \n",
    "                                                     end_date=state_dict['end_date'], \n",
    "                                                     precip_table_name='weather'\n",
    "                                                    )\n",
    "        \n",
    "        state_dict.update({\"precip_table_name\":precip_table_name})\n",
    "\n",
    "        session.close()\n",
    "        return state_dict\n",
    "\n",
    "    def generate_feature_table_task(state_dict:dict)-> dict: \n",
    "        from parallel_udf import generate_feature_table\n",
    "        print('Generating feature table for all stations.')\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        \n",
    "        _ = session.sql('USE WAREHOUSE '+compute_parameters['fe_warehouse']).collect()\n",
    "\n",
    "        clone_table_name = 'TRIPS_CLONE_'+state_dict[\"model_id\"]\n",
    "        state_dict.update({\"clone_table_name\":clone_table_name})\n",
    "        \n",
    "        _ = session.sql('CREATE OR REPLACE TABLE '+clone_table_name+\" CLONE \"+state_dict[\"trips_table_name\"]).collect()\n",
    "        _ = session.sql('CREATE TAG IF NOT EXISTS model_id_tag').collect()\n",
    "        _ = session.sql(\"ALTER TABLE \"+clone_table_name+\" SET TAG model_id_tag = '\"+state_dict[\"model_id\"]+\"'\").collect()\n",
    "        \n",
    "        feature_table_name = generate_feature_table(session=session, \n",
    "                                                    clone_table_name=state_dict[\"clone_table_name\"], \n",
    "                                                    feature_table_name='TRIPS_FEATURES_'+state_dict[\"model_id\"], \n",
    "                                                    holiday_table_name=state_dict[\"holiday_table_name\"],\n",
    "                                                    precip_table_name=state_dict[\"precip_table_name\"]\n",
    "                                                   )\n",
    "        state_dict.update({\"feature_table_name\":feature_table_name})\n",
    "\n",
    "        session.close()\n",
    "        return state_dict\n",
    "    \n",
    "    def bulk_train_predict_task(state_dict:dict)-> dict: \n",
    "        from parallel_udf import train_predict_feature_table\n",
    "        print('Running bulk training and inference on feature table')\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        \n",
    "        _ = session.sql('USE WAREHOUSE '+compute_parameters['train_warehouse']).collect()\n",
    "        \n",
    "        pred_table_name = train_predict_feature_table(session=session, \n",
    "                                                      station_train_pred_udf_name=state_dict[\"model_udf_name\"], \n",
    "                                                      feature_table_name=state_dict[\"feature_table_name\"], \n",
    "                                                      pred_table_name='PRED_'+state_dict[\"model_id\"]\n",
    "                                                     )\n",
    "        \n",
    "        state_dict.update({\"pred_table_name\":pred_table_name})\n",
    "        session.close()\n",
    "        return state_dict\n",
    "    \n",
    "    def deploy_eval_udf_task(state_dict:dict)-> dict:\n",
    "        from model_eval import deploy_eval_udf\n",
    "        print('Deploying udf for model evaluation.')\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        eval_model_udf_name = deploy_eval_udf(session=session, \n",
    "                                              model_stage_name=state_dict['model_stage_name']\n",
    "                                              )\n",
    "                \n",
    "        state_dict.update({\"eval_model_udf_name\":eval_model_udf_name})\n",
    "\n",
    "        session.close()\n",
    "        return state_dict\n",
    "\n",
    "    def eval_station_preds_task(state_dict:dict)-> dict:\n",
    "        from model_eval import evaluate_station_predictions\n",
    "        print('Running eval UDF for model output')\n",
    "        session, compute_parameters = snowpark_connect()\n",
    "        \n",
    "        _ = session.sql('USE WAREHOUSE '+compute_parameters['fe_warehouse']).collect()\n",
    "\n",
    "        eval_table_name = evaluate_station_predictions(session=session, \n",
    "                                                       pred_table_name=state_dict['pred_table_name'],\n",
    "                                                       eval_model_udf_name=state_dict['eval_model_udf_name'],\n",
    "                                                       eval_table_name='EVAL_'+state_dict[\"model_id\"]\n",
    "                                                       )\n",
    "        state_dict.update({\"eval_table_name\":eval_table_name})\n",
    "\n",
    "        session.close()\n",
    "        return state_dict                                               \n",
    "    \n",
    "    #Task order\n",
    "    state_dict = snowpark_database_setup(state_dict)\n",
    "    #state_dict = incremental_elt_task(state_dict, files_to_download)\n",
    "    \n",
    "    state_dict = deploy_model_udf_task(state_dict)\n",
    "    #for testing\n",
    "    #state_dict.update({\"model_udf_name\":'station_train_predict_udf'})\n",
    "    \n",
    "    state_dict = materialize_holiday_task(state_dict)\n",
    "    state_dict = materialize_precip_task(state_dict)\n",
    "    #for testing\n",
    "    state_dict.update({\"holiday_table_name\":'HOLIDAYS'})\n",
    "    state_dict.update({\"precip_table_name\":'WEATHER'})\n",
    "    \n",
    "    state_dict = generate_feature_table_task(state_dict) \n",
    "    #for testing\n",
    "    #state_dict.update({\"feature_table_name\":'TRIPS_FEATURES_6BFB8E62_811A_11EC_8C7C_ACDE48001122'})\n",
    "    #state_dict.update({\"model_id\":'6BFB8E62_811A_11EC_8C7C_ACDE48001122'})\n",
    "    \n",
    "    state_dict = bulk_train_predict_task(state_dict)\n",
    "    #for testing\n",
    "    #state_dict.update({\"pred_table_name\":'PRED_6BFB8E62_811A_11EC_8C7C_ACDE48001122'})\n",
    "\n",
    "    #state_dict = deploy_eval_udf_task(state_dict)\n",
    "    #state_dict.update({\"eval_model_udf_name\":'eval_model_output_udf'})\n",
    "\n",
    "    #state_dict = eval_station_preds_task(state_dict)        \n",
    "\n",
    "    return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bulk_load_internal import bulk_load\n",
    "#bulk_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "state_dict = snowpark_citibike_ml_taskflow(files_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowpark_connection import snowpark_connect\n",
    "\n",
    "session, compute_parameters = snowpark_connect('creds.json')\n",
    "#session.table(state_dict['eval_table_name']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.table(state_dict['eval_table_name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowpark_connection import snowpark_connect\n",
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import udf\n",
    "import ast\n",
    "session, compute_parameters = snowpark_connect('creds.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_df = session.table(state_dict['clone_table_name']) #.filter(F.col('START_STATION_ID') == '3631')\n",
    "feature_df = session.table(state_dict['feature_table_name']) #.filter(F.col('STATION_ID') == '519')\n",
    "pred_df = session.table(state_dict['pred_table_name']) #.filter(F.col('START_STATION_ID') == '3631')\n",
    "holiday_df = session.table(state_dict['holiday_table_name'])\n",
    "precip_df = session.table(state_dict['precip_table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.filter(F.col('PRED') == 'NULL').select('STATION_ID').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.filter(F.col('STATION_ID') == '3668').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_list = feature_df.select('STATION_ID', F.call_udf('station_train_predict_udf', \n",
    "#                                                          'INPUT_DATA', \n",
    "#                                                           'INPUT_COLUMN_LIST', \n",
    "#                                                           'TARGET_COLUMN', \n",
    "#                                                           F.lit(10)).alias('OUTPUT_DATA')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "# import pandas as pd\n",
    "\n",
    "# for row in range(len(output_list)):\n",
    "#     tempdf = pd.DataFrame(data = ast.literal_eval(output_list[row]['OUTPUT_DATA'])[0], \n",
    "#                                 columns=ast.literal_eval(output_list[row]['OUTPUT_DATA'])[1]\n",
    "#                                 )\n",
    "#     tempdf['STATION_ID'] = str(output_list[row]['STATION_ID'])\n",
    "#     print(tempdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = Window.partitionBy(F.col('STATION_ID')).orderBy(F.col('DATE').asc())\n",
    "\n",
    "# feature_df = clone_df.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "#                              F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "#                      .groupBy(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "#                         .count()\\\n",
    "#                      .withColumn('LAG_1', F.lag(F.col('COUNT'), offset=1, default_value=None).over(window))\\\n",
    "#                      .withColumn('LAG_7', F.lag(F.col('COUNT'), offset=7, default_value=None).over(window))\\\n",
    "#                         .na.drop()\\\n",
    "#                      .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "#                      .join(precip_df, 'DATE', 'inner')\n",
    "\n",
    "# feature_column_list = feature_df.columns\n",
    "# feature_column_list.remove('\\\"STATION_ID\\\"')\n",
    "# feature_column_list = [f.replace('\\\"', \"\") for f in feature_column_list]\n",
    "# feature_column_array = F.array_construct(*[F.lit(x) for x in feature_column_list])\n",
    "\n",
    "# feature_df_stuffed = feature_df.groupBy(F.col('STATION_ID'))\\\n",
    "#                                .agg(F.array_agg(F.array_construct(*feature_column_list)).alias('INPUT_DATA'))\\\n",
    "#                                .withColumn('INPUT_COLUMN_LIST', feature_column_array)\\\n",
    "#                                .withColumn('TARGET_COLUMN', F.lit('COUNT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df_stuffed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = ast.literal_eval(feature_df_stuffed.select('INPUT_DATA').collect()[0][0])\n",
    "# len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data2 = ast.literal_eval(session.table(state_dict['feature_table_name']).filter(F.col('STATION_ID') == '3631').select('INPUT_DATA').collect()[0][0])\n",
    "# len(input_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df = session.table(state_dict['feature_table_name']).filter(F.col('STATION_ID') == '290')\n",
    "\n",
    "# import ast\n",
    "# import pandas as pd\n",
    "# input_data = ast.literal_eval(feature_df.select('INPUT_DATA').collect()[0][0])\n",
    "# input_columns = ast.literal_eval(feature_df.select('INPUT_COLUMN_LIST').collect()[0][0])\n",
    "# target_column = feature_df.select('TARGET_COLUMN').collect()[0][0]\n",
    "# station_id = feature_df.select('STATION_ID').collect()[0][0]\n",
    "# max_epochs=10\n",
    "\n",
    "# df = pd.DataFrame(input_data, columns = input_columns)\n",
    "\n",
    "# if len(df) < 365*2:\n",
    "#         df['PRED'] = 'NULL'\n",
    "# else:\n",
    "#     print('big')\n",
    "#     feature_columns = input_columns.copy()\n",
    "#     feature_columns.remove('DATE')\n",
    "#     feature_columns.remove(target_column)\n",
    "#     print(feature_columns)\n",
    "    \n",
    "#     from torch import tensor\n",
    "#     from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "#     model = TabNetRegressor()\n",
    "\n",
    "#     #cutpoint = round(len(df)*(train_valid_split/100))\n",
    "#     cutpoint = 365\n",
    "\n",
    "#     ##NOTE: in order to do train/valid split on time-based portion the input data must be sorted by date    \n",
    "#     df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "#     df = df.sort_values(by='DATE', ascending=True)\n",
    "\n",
    "#     y_valid = df[target_column][-cutpoint:].values.reshape(-1, 1)\n",
    "#     X_valid = df[feature_columns][-cutpoint:].values\n",
    "#     y_train = df[target_column][:-cutpoint].values.reshape(-1, 1)\n",
    "#     X_train = df[feature_columns][:-cutpoint].values\n",
    "#     print(station_id, y_valid.shape, X_valid.shape, y_train.shape, X_train.shape)\n",
    "\n",
    "#     model.fit(\n",
    "#         X_train, y_train,\n",
    "#         eval_set=[(X_valid, y_valid)],\n",
    "#         max_epochs=max_epochs,\n",
    "#         patience=100,\n",
    "#         batch_size=1024, \n",
    "#         virtual_batch_size=128,\n",
    "#         num_workers=0,\n",
    "#         drop_last=False)\n",
    "\n",
    "\n",
    "#     df['PRED'] = model.predict(tensor(df[feature_columns].values))\n",
    "#     df['DATE'] = df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "#     df = pd.concat([df, pd.DataFrame(model.explain(df[feature_columns].values)[0], \n",
    "#                            columns = feature_columns).add_prefix('EXPL_').round(2)], axis=1)\n",
    "    \n",
    "# from station_train_predict import station_train_predict_func as stpf\n",
    "# output_list = stpf(station_id=station_id,\n",
    "#                                input_data=input_data,\n",
    "#                                input_columns_str=input_columns_str,\n",
    "#                                target_column=target_column,\n",
    "#                                train_valid_split=train_valid_split,\n",
    "#                                max_epochs=max_epochs)\n",
    "\n",
    "# output_list\n",
    "\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #show how many rows are stuffed\n",
    "# import ast\n",
    "\n",
    "# feature_df2 = session.table(state_dict['feature_table_name'])\n",
    "\n",
    "# station_list = list(feature_df2.select('STATION_ID').toPandas()['STATION_ID'].values)\n",
    "# for station in station_list:\n",
    "#     input_data = ast.literal_eval(feature_df2.filter(F.col('STATION_ID') == station).select('INPUT_DATA').collect()[0][0])\n",
    "#     print(station, len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone_df.select('STARTTIME', 'START_STATION_ID')\\\n",
    "#       .withColumn('DATE', F.call_builtin('DATE_TRUNC', (agg_period, F.col('STARTTIME'))))\\\n",
    "#       .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "#       .join(precip_df, 'DATE', 'inner')\\\n",
    "#       .groupBy(F.col('DATE'), F.col('START_STATION_ID'))\\\n",
    "#         .count()\\\n",
    "#       .groupBy(F.col('START_STATION_ID'))\\\n",
    "#         .count()\\\n",
    "#       .sort('COUNT', ascending=False)\\\n",
    "#       .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_list = feature_df.select(F.call_udf('station_train_predict_udf', \n",
    "#                                           'STATION_ID',\n",
    "#                                           'INPUT_DATA', \n",
    "#                                           'INPUT_COLUMN_NAMES', \n",
    "#                                           'TARGET_COLUMN', \n",
    "#                                           F.lit(1), \n",
    "#                                           F.lit(10)).alias('OUTPUT_DATA')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.sql('USE WAREHOUSE XXXX4L').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_list = feature_df.select(F.call_udf('station_train_predict_udf', \n",
    "#                                           'STATION_ID',\n",
    "#                                           'INPUT_DATA', \n",
    "#                                           'INPUT_COLUMN_NAMES', \n",
    "#                                           'TARGET_COLUMN', \n",
    "#                                           F.lit(1), \n",
    "#                                           F.lit(10)).alias('OUTPUT_DATA')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.sql('USE WAREHOUSE load_wh').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_df.select(F.col('STATION_ID').alias('START_STATION_ID')).sort('START_STATION_ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clone_df.select('STARTTIME', 'START_STATION_ID')\\\n",
    "#       .withColumn('DATE', F.call_builtin('DATE_TRUNC', (agg_period, F.col('STARTTIME'))))\\\n",
    "#       .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "#       .join(precip_df, 'DATE', 'inner')\\\n",
    "#       .groupBy(F.col('DATE'), F.col('START_STATION_ID'))\\\n",
    "#         .count()\\\n",
    "#       .groupBy(F.col('START_STATION_ID'))\\\n",
    "#         .count()\\\n",
    "#       .join(feature_df.select(F.col('STATION_ID').alias('START_STATION_ID')), 'START_STATION_ID')\\\n",
    "#       .sort('START_STATION_ID', ascending=True)\\\n",
    "#       .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test predict func\n",
    "# import ast\n",
    "# import pandas as pd\n",
    "# input_data = ast.literal_eval(feature_df.limit(1).select('INPUT_DATA').collect()[0][0])\n",
    "# station_id = ast.literal_eval(feature_df.limit(1).select('STATION_ID').collect()[0][0])\n",
    "# input_columns_str = feature_df.limit(1).select('INPUT_COLUMN_NAMES').collect()[0][0]\n",
    "# target_column = feature_df.limit(1).select('TARGET_COLUMN').collect()[0][0]\n",
    "# train_valid_split=20\n",
    "# max_epochs=10\n",
    "\n",
    "# input_columns = input_columns_str.split(' ')\n",
    "# feature_columns = input_columns.copy()\n",
    "# feature_columns.remove('DATE')\n",
    "# feature_columns.remove(target_column)\n",
    "\n",
    "# df = pd.DataFrame(input_data, columns = input_columns)\n",
    "# df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "# df = df.sort_values(by='DATE', ascending=True)\n",
    "# df['DATE'] = df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "# cutpoint = round(len(df)*(train_valid_split/100))\n",
    "# y_valid = df[target_column][-cutpoint:].values.reshape(-1, 1)\n",
    "# X_valid = df[feature_columns][-cutpoint:].values\n",
    "# y_train = df[target_column][:-cutpoint].values.reshape(-1, 1)\n",
    "# X_train = df[feature_columns][:-cutpoint].values\n",
    "\n",
    "# from station_train_predict import station_train_predict_func as stpf\n",
    "# output_list = stpf(station_id=station_id,\n",
    "#                                input_data=input_data,\n",
    "#                                input_columns_str=input_columns_str,\n",
    "#                                target_column=target_column,\n",
    "#                                train_valid_split=train_valid_split,\n",
    "#                                max_epochs=max_epochs)\n",
    "\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.sql('USE WAREHOUSE '+compute_parameters['fe_warehouse']).collect()\n",
    "\n",
    "# output_list = session.table('TRIPS_FEATURES_6BFB8E62_811A_11EC_8C7C_ACDE48001122')\\\n",
    "#                        .select(F.call_udf('station_train_predict_udf', \n",
    "#                                           'STATION_ID',\n",
    "#                                           'INPUT_DATA', \n",
    "#                                           'INPUT_COLUMN_NAMES', \n",
    "#                                           'TARGET_COLUMN', \n",
    "#                                           F.lit(train_valid_split), \n",
    "#                                           F.lit(max_epochs)).alias('OUTPUT_DATA')).collect()\n",
    "\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast.literal_eval(output_list[row]['OUTPUT_DATA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "\n",
    "# for row in range(len(output_list)):\n",
    "#     df = pd.concat([df, \n",
    "#                     pd.DataFrame(data = ast.literal_eval(output_list[row]['OUTPUT_DATA'])[0], \n",
    "#                                 columns=ast.literal_eval(output_list[row]['OUTPUT_DATA'])[1]\n",
    "#                                 )\n",
    "#                    ], \n",
    "#                    axis=0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_list = list(feature_df.select('STATION_ID').toPandas()['STATION_ID'].values)\n",
    "# for station in station_list:\n",
    "#     input_data = ast.literal_eval(feature_df.filter(F.col('STATION_ID') == station).select('INPUT_DATA').collect()[0][0])\n",
    "#     print(stpf(station_id=station,\n",
    "#                                input_data=input_data,\n",
    "#                                input_columns_str=input_columns_str,\n",
    "#                                target_column=target_column,\n",
    "#                                train_valid_split=train_valid_split,\n",
    "#                                max_epochs=max_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_list = list(feature_df.select('STATION_ID').toPandas()['STATION_ID'].values)\n",
    "# for station in station_list:\n",
    "#     input_data = ast.literal_eval(feature_df.filter(F.col('STATION_ID') == station).select('INPUT_DATA').collect()[0][0])\n",
    "\n",
    "#     print(station+' '+str(len(input_data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
